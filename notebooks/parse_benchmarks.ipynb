{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3224a187",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e04e42de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_benchmark_logs(input_file):\n",
    "    lines = []\n",
    "    with open(input_file) as file:\n",
    "        lines = file.readlines()\n",
    "        \n",
    "    index = -1\n",
    "    lines_dict = {}\n",
    "\n",
    "    for line in lines:\n",
    "        if 'Tokenizer' in line:\n",
    "            index = index+1\n",
    "            lines_dict[index] = []\n",
    "\n",
    "        if index!=-1:\n",
    "            lines_dict[index].append(line)\n",
    "            \n",
    "    \n",
    "    final_entries = []\n",
    "\n",
    "    for key, lines in lines_dict.items():\n",
    "        words_needed = {'Model':'was successfully restored',\n",
    "                        'Tokenizer':'Tokenizer',\n",
    "                        'Dataset':'Reading Manifest',\n",
    "                        'WER':'Greedy WER/CER', \n",
    "                        'WER_LM':'WER/CER with beam search decoding and N-gram model'}\n",
    "\n",
    "        lines_filtered = {}\n",
    "        for line in lines:\n",
    "            for key, value in words_needed.items():\n",
    "                if value in line:\n",
    "                    lines_filtered[key] = line.strip()\n",
    "\n",
    "        tokenizer =''\n",
    "        tokens = ''\n",
    "        model_path = ''\n",
    "        model_name = ''\n",
    "        model_type = ''\n",
    "        dataset_manifest_path = ''\n",
    "        dataset = ''\n",
    "        wer = ''\n",
    "        cer = ''\n",
    "        wer_lm = ''\n",
    "        cer_lm = ''\n",
    "        alpha =''\n",
    "        beta =''\n",
    "        beam =''\n",
    "\n",
    "        for key, value in lines_filtered.items():\n",
    "            if key == \"Tokenizer\":\n",
    "                tokenizer = value.split(' ')[-5]\n",
    "                tokens = value.split(' ')[-2]\n",
    "    #             print(tokenizer)\n",
    "    #             print(tokens)\n",
    "\n",
    "            if key == 'Model':\n",
    "                model_path = value.split(' ')[-1][0:-1]\n",
    "                model_name = model_path.split('/')[-1]\n",
    "                model_type = value.split(']')[-1].split(' ')[2]\n",
    "    #             print(model_path)\n",
    "    #             print(model_name)\n",
    "\n",
    "            if key == 'Dataset':\n",
    "                dataset_manifest_path = value.split(' ')[2]\n",
    "                dataset = \"_\".join(dataset_manifest_path.split('/')[-1].split('.')[0].split('_')[0:-1])\n",
    "    #             print(dataset)\n",
    "    #             print(\"_\".join(dataset.split('/')[-1].split('.')[0].split('_')[0:-1]))\n",
    "\n",
    "            if key == \"WER\":\n",
    "                wer_ = value.split('=')[-1]\n",
    "                wer, cer = wer_.split('/')\n",
    "    #             print(wer, cer)\n",
    "\n",
    "            if key == \"WER_LM\":\n",
    "                wer_lm_ = value.split('=')[-1]\n",
    "                wer_lm, cer_lm = wer_lm_.split('/')\n",
    "\n",
    "                beam = value.split(',')[0].split('=')[-1]\n",
    "                alpha = value.split(',')[1].split('=')[-1]\n",
    "                beta = value.split(',')[2].split('=')[-1].split(':')[0]\n",
    "\n",
    "        final_entries.append({\n",
    "            'tokenizer': tokenizer,\n",
    "            'tokens' : tokens,\n",
    "            'model_path' : model_path,\n",
    "            'model_name' : model_name,\n",
    "            'model_type' : model_type,\n",
    "            'dataset_manifest_path' : dataset_manifest_path,\n",
    "            'dataset' : dataset,\n",
    "            'wer' : wer,\n",
    "            'cer' : cer,\n",
    "            'wer_lm' : wer_lm,\n",
    "            'cer_lm' : cer_lm,\n",
    "            'beam' : beam,\n",
    "            'alpha' : alpha,\n",
    "            'beta' : beta\n",
    "        })\n",
    "    return final_entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "da692cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = parse_benchmark_logs('../scripts/inference/nohup.out')\n",
    "df = pd.DataFrame(logs)\n",
    "df.to_csv('benchmarks.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
